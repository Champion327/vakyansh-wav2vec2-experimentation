gpus_to_simulate : 64 # For base model we can simulate 64 GPUs
num_workers: 6
conv_feature_layers: '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] * 2'
final_dim: 256
latent_vars: 320
latent_groups: 2
latent_temp: '(2,0.5,0.999995)'
log_keys: '["prob_perplexity","code_perplexity","temp"]' 
optimizer : adam 
adam_betas : '(0.9, 0.98)'
adam_eps : 1e-06
lr_scheduler: polynomial_decay 
total_num_update: 400000
lr: 0.0005
warmup_updates: 32000 
mask:
  selection : static  
  other : 0 
  length : 10  
  prob : 0.65
encoder_layerdrop: 0.05
dropout_input: 0.1 
dropout_features: 0.1 
feature_grad_mult: 0.1
loss_weights: '[0.1, 10]'
conv_pos: 128 
conv_pos_groups: 16 
num_negatives: 100 
cross_sample_negatives: 0
max_sample_size: 250000
min_sample_size: 32000
dropout: 0.1 
attention_dropout: 0.1 
weight_decay: 0.01 
max_tokens: 1400000 
max_update: 400000 
ddp_backend: no_c10d

