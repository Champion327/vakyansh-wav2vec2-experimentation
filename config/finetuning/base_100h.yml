# @package _group_

common:
    no_progress_bar: false
    log_interval: 500
    log_format: json
    tensorboard_logdir: 
    wandb_project: 
    azureml_logging: false
    seed: 2337
    cpu: false
    tpu: false
    bf16: false
    memory_efficient_bf16: false
    fp16: true
    memory_efficient_fp16: false
    fp16_no_flatten_grads: false
    fp16_init_scale: 128
    fp16_scale_window: 
    fp16_scale_tolerance: 0.0
    min_loss_scale: 0.0001
    threshold_loss_scale: 
    user_dir: 
    empty_cache_freq: 0
    all_gather_list_size: 16384
    model_parallel_size: 1
    quantization_config_path: 
    profile: false
    reset_logging: false
    suppress_crashes: false
    use_plasma_view: false
    plasma_path: '/tmp/plasma'

checkpoint:
    finetune_from_model: 
    reset_dataloader: false
    reset_lr_scheduler: false
    reset_meters: false
    reset_optimizer: false
    optimizer_overrides: '{}'
    save_interval: 1
    save_interval_updates: 5000
    keep_interval_updates: 1
    keep_interval_updates_pattern: -1
    keep_last_epochs: -1
    keep_best_checkpoints: -1
    no_save: false
    no_epoch_checkpoints: true
    no_last_checkpoints: false
    no_save_optimizer_state: false
    best_checkpoint_metric: wer
    maximize_best_checkpoint_metric: false
    patience: -1
    checkpoint_suffix: 
    checkpoint_shard_count: 1
    load_checkpoint_on_all_dp_ranks: false
    write_checkpoints_asynchronously: false
    model_parallel_size: 1
  

task:
  _name: audio_pretraining
  data: ???
  normalize: false
  labels: ltr

dataset:
    num_workers: 1
    skip_invalid_size_inputs_valid_test: false
    max_tokens: 3200000
    batch_size: 
    required_batch_size_multiple: 8
    required_seq_len_multiple: 1
    dataset_impl: 
    data_buffer_size: 10
    train_subset: train
    valid_subset: valid
    validate_interval: 1
    validate_interval_updates: 0
    validate_after_updates: 0
    fixed_validation_seed: 
    disable_validation: false
    max_tokens_valid: 3200000
    batch_size_valid: 
    max_valid_steps: 
    curriculum: 0
    gen_subset: test
    num_shards: 1
    shard_id: 0
  

distributed_training:
    distributed_world_size: 8
    distributed_rank: 0
    distributed_backend: nccl
    distributed_init_method: tcp://localhost:14891
    distributed_port: -1
    device_id: 0
    distributed_no_spawn: false
    ddp_backend: c10d
    bucket_cap_mb: 25
    fix_batches_to_gpus: false
    find_unused_parameters: true
    fast_stat_sync: false
    heartbeat_timeout: -1
    broadcast_buffers: false
    slowmo_momentum: 
    slowmo_algorithm: LocalSGD
    localsgd_frequency: 3
    nprocs_per_node: 1
    pipeline_model_parallel: false
    pipeline_balance: 
    pipeline_devices: 
    pipeline_chunks: 0
    pipeline_encoder_balance: 
    pipeline_encoder_devices: 
    pipeline_decoder_balance: 
    pipeline_decoder_devices: 
    pipeline_checkpoint: never
    zero_sharding: 
    fp16: true
    memory_efficient_fp16: false
    tpu: true
    no_reshard_after_forward: false
    fp32_reduce_scatter: false
    cpu_offload: false


criterion:
  _name: ctc
  zero_infinity: true

optimization:
    max_update: 80000
    lr: [0.00003]
    sentence_avg: true
    update_freq: [4]
    use_bmuf: false
    max_epoch: 0
    stop_min_lr: -1
    stop_time_hours: 0
    clip_norm: 25.0

optimizer:
  _name: adam
  adam_betas: (0.9,0.98)
  adam_eps: 1e-08

lr_scheduler:
  _name: tri_stage
  phase_ratio: [0.1, 0.4, 0.5]
  final_lr_scale: 0.05

model:
  _name: wav2vec_ctc
  w2v_path: ???
  apply_mask: true
  mask_prob: 0.65
  mask_channel_prob: 0.5
  mask_channel_length: 64
  layerdrop: 0.1
  activation_dropout: 0.1
  feature_grad_mult: 0.0
  freeze_finetune_updates: 0

